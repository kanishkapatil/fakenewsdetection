
# from flask import Blueprint, request, jsonify
# import pickle
# import os
# import numpy as np
# import requests
# import urllib3
# from newspaper import Article, Config
# from fpdf import FPDF
# from flask import send_file
# import io

# # Suppress insecure request warnings (caused by verify=False)
# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# predict_bp = Blueprint("predict", __name__)

# # --- Configuration ---
# # Use your actual key here
# GOOGLE_API_KEY = "YAIzaSyCaFdRx30Pfow1od2o7lAcGsfgQjJ8NJ9E" 
# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# MODEL_PATH = os.path.join(BASE_DIR, "ml_dl", "fake_news_model.pkl")
# VECTORIZER_PATH = os.path.join(BASE_DIR, "ml_dl", "tfidf_vectorizer.pkl")

# # Load model & vectorizer
# with open(MODEL_PATH, "rb") as f: model = pickle.load(f)
# with open(VECTORIZER_PATH, "rb") as f: vectorizer = pickle.load(f)

# def get_google_fact_check(query):
#     url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search"
#     params = {"query": query[:200], "key": GOOGLE_API_KEY}
#     try:
#         response = requests.get(url, params=params)
#         data = response.json()
#         if "claims" in data and len(data["claims"]) > 0:
#             claim = data["claims"][0]
#             review = claim["claimReview"][0]
#             return {
#                 "found": True,
#                 "rating": review["textualRating"],
#                 "source": review["publisher"]["name"],
#                 "url": review["url"]
#             }
#     except: pass
#     return {"found": False}


# @predict_bp.route("/generate_report", methods=["POST"])
# def generate_report():
#     data = request.get_json()
    
#     # Extract data from the request
#     text = data.get("text", "No text provided")
#     prediction = data.get("prediction", "Unknown")
#     confidence = data.get("confidence", 0)
#     keywords = data.get("keywords", [])
#     verdict = data.get("verdict", "N/A") # From Google Fact Check

#     # Create PDF
#     pdf = FPDF()
#     pdf.add_page()
    
#     # Title
#     pdf.set_font("Arial", 'B', 24)
#     pdf.set_text_color(37, 99, 235) # Blue
#     pdf.cell(200, 20, "TRUTH ENGINE ANALYSIS REPORT", ln=True, align='C')
    
#     # Result Banner
#     pdf.ln(10)
#     pdf.set_fill_color(240, 240, 240)
#     pdf.set_font("Arial", 'B', 16)
#     pdf.set_text_color(0, 0, 0)
#     pdf.cell(0, 15, f"FINAL VERDICT: {prediction.upper()}", ln=True, align='C', fill=True)
    
#     # Details
#     pdf.ln(10)
#     pdf.set_font("Arial", 'B', 12)
#     pdf.cell(0, 10, f"AI Confidence Score: {confidence * 100}%", ln=True)
#     pdf.cell(0, 10, f"External Fact-Check: {verdict}", ln=True)
#     pdf.cell(0, 10, f"Key Indicators Identified: {', '.join(keywords)}", ln=True)
    
#     # Analyzed Text
#     pdf.ln(10)
#     pdf.set_font("Arial", 'B', 14)
#     pdf.cell(0, 10, "Analyzed Content Snippet:", ln=True)
#     pdf.set_font("Arial", '', 10)
#     pdf.multi_cell(0, 5, text[:1000] + "...") # First 1000 characters
    
#     # Footer
#     pdf.set_y(-30)
#     pdf.set_font("Arial", 'I', 8)
#     pdf.cell(0, 10, "Generated by Truth Engine AI - Forensic News Analysis System", align='C')

#     # Save to byte stream to send to browser
#     output = io.BytesIO()
#     pdf_output = pdf.output(dest='S').encode('latin1')
#     output.write(pdf_output)
#     output.seek(0)

#     return send_file(output, mimetype='application/pdf', as_attachment=True, download_name="Truth_Report.pdf")

# @predict_bp.route("/scrape", methods=["POST"])
# def scrape_url():
#     data = request.get_json()
#     url = data.get("url")
    
#     # Advanced Headers to look exactly like a real Chrome Browser
#     headers = {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#         'Accept-Language': 'en-US,en;q=0.9',
#         'Accept-Encoding': 'gzip, deflate, br',
#         'Connection': 'keep-alive',
#         'Upgrade-Insecure-Requests': '1',
#         'Cache-Control': 'max-age=0'
#     }

#     try:
#         # Use a session to maintain cookies
#         session = requests.Session()
#         # We use a timeout of 20 to give slow news sites time to respond
#         response = session.get(url, headers=headers, timeout=20, verify=False, allow_redirects=True)
        
#         if response.status_code != 200:
#             return jsonify({"error": f"Site blocked us (Status: {response.status_code})"}), 400

#         article = Article(url)
#         article.set_html(response.text)
#         article.parse()
        
#         if not article.text or len(article.text) < 50:
#              # Fallback: Sometimes title is there even if text is blocked
#              if article.title:
#                  return jsonify({"text": "Content blocked by site security, but title found: " + article.title, "title": article.title})
#              return jsonify({"error": "No text found on page."}), 400
             
#         return jsonify({"text": article.text, "title": article.title})
        
#     except Exception as e:
#         print(f"DEBUG ERROR: {str(e)}")
#         return jsonify({"error": "Connection failed. Try a different news source."}), 500

# @predict_bp.route("/predict", methods=["POST"])
# def predict():
#     data = request.get_json()
#     text = data.get("text", "")
    
#     if not text:
#         return jsonify({"error": "No text provided"}), 400

#     # 1. External Fact Check
#     google_result = get_google_fact_check(text)
    
#     # 2. ML Prediction
#     X = vectorizer.transform([text])
#     pred = int(model.predict(X)[0])
#     prob = model.predict_proba(X)[0]
    
#     # 3. Keywords
#     feature_names = vectorizer.get_feature_names_out()
#     tfidf_scores = X.toarray()[0]
#     top_indices = tfidf_scores.argsort()[-5:][::-1]
#     top_keywords = [feature_names[i] for i in top_indices if tfidf_scores[i] > 0]

#     response = {
#         "prediction": "Real" if pred == 1 else "Fake",
#         "confidence": round(float(np.max(prob)), 2),
#         "top_keywords": top_keywords,
#         "external_verification": None
#     }

#     if google_result["found"]:
#         response["external_verification"] = {
#             "verdict": google_result["rating"],
#             "provider": google_result["source"],
#             "link": google_result["url"]
#         }
#         # Adjust verdict based on external facts
#         if any(word in google_result["rating"].lower() for word in ["false", "fake", "misleading", "incorrect"]):
#             response["prediction"] = "Fake"
#             response["confidence"] = 1.0

#     return jsonify(response)





# from flask import Blueprint, request, jsonify, send_file
# import pickle
# import os
# import numpy as np
# import requests
# import urllib3
# from newspaper import Article
# from fpdf import FPDF
# import io

# # Suppress insecure request warnings
# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# predict_bp = Blueprint("predict", __name__)

# # --- Configuration ---
# GOOGLE_API_KEY = "YAIzaSyCaFdRx30Pfow1od2o7lAcGsfgQjJ8NJ9E" 
# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# MODEL_PATH = os.path.join(BASE_DIR, "ml_dl", "fake_news_model.pkl")
# VECTORIZER_PATH = os.path.join(BASE_DIR, "ml_dl", "tfidf_vectorizer.pkl")

# # Load model & vectorizer
# with open(MODEL_PATH, "rb") as f: model = pickle.load(f)
# with open(VECTORIZER_PATH, "rb") as f: vectorizer = pickle.load(f)

# def get_google_fact_check(query):
#     url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search"
#     params = {"query": query[:200], "key": GOOGLE_API_KEY}
#     try:
#         response = requests.get(url, params=params)
#         data = response.json()
#         if "claims" in data and len(data["claims"]) > 0:
#             claim = data["claims"][0]
#             review = claim["claimReview"][0]
#             return {
#                 "found": True,
#                 "rating": review["textualRating"],
#                 "source": review["publisher"]["name"],
#                 "url": review["url"]
#             }
#     except: pass
#     return {"found": False}

# def clean_for_pdf(text):
#     """Encodes text to Latin-1, ignoring characters that can't be represented."""
#     if not text:
#         return ""
#     # Remove characters that FPDF's standard fonts (Arial/Helvetica) cannot handle
#     return str(text).encode('latin-1', 'ignore').decode('latin-1')

# @predict_bp.route("/generate_report", methods=["POST"])
# def generate_report():
#     try:
#         data = request.get_json()
        
#         # Extract data
#         text = data.get("text", "No text provided")
#         prediction = data.get("prediction", "Unknown")
#         confidence = data.get("confidence", 0)
#         keywords = data.get("keywords", [])
#         # Handle the case where external_verification might be nested or missing
#         ext_ver = data.get("external_verification")
#         verdict = ext_ver.get("verdict", "N/A") if ext_ver else "N/A"

#         # Create PDF
#         pdf = FPDF()
#         pdf.add_page()
        
#         # Header - Blue Title
#         pdf.set_font("Arial", 'B', 24)
#         pdf.set_text_color(37, 99, 235) 
#         pdf.cell(190, 20, "TRUTH ENGINE ANALYSIS REPORT", ln=True, align='C')
        
#         # Result Banner
#         pdf.ln(10)
#         pdf.set_fill_color(240, 240, 240)
#         pdf.set_font("Arial", 'B', 16)
#         pdf.set_text_color(0, 0, 0)
#         pdf.cell(0, 15, f"FINAL VERDICT: {clean_for_pdf(prediction.upper())}", ln=True, align='C', fill=True)
        
#         # Key Details Section
#         pdf.ln(10)
#         pdf.set_text_color(50, 50, 50)
#         pdf.set_font("Arial", 'B', 12)
#         pdf.cell(0, 10, f"AI Confidence Score: {round(confidence * 100, 2)}%", ln=True)
#         pdf.cell(0, 10, f"External Fact-Check: {clean_for_pdf(verdict)}", ln=True)
        
#         # Keywords
#         kw_str = ", ".join(keywords) if keywords else "None detected"
#         pdf.multi_cell(0, 10, f"Key Indicators Identified: {clean_for_pdf(kw_str)}")
        
#         # Analyzed Content Section
#         pdf.ln(10)
#         pdf.set_font("Arial", 'B', 14)
#         pdf.set_text_color(37, 99, 235)
#         pdf.cell(0, 10, "Analyzed Content Snippet:", ln=True)
#         pdf.set_font("Arial", '', 10)
#         pdf.set_text_color(80, 80, 80)
        
#         # Clean the long text snippet
#         content_snippet = clean_for_pdf(text[:1200]) + "..."
#         pdf.multi_cell(0, 6, content_snippet)
        
#         # Footer
#         pdf.set_y(-30)
#         pdf.set_font("Arial", 'I', 8)
#         pdf.set_text_color(150, 150, 150)
#         pdf.cell(0, 10, "Generated by Truth Engine AI - Forensic News Analysis System", align='C')

#         # Generate output as a string then convert to bytes for buffer
#         # Using 'S' destination returns the document as a string
#         pdf_str = pdf.output(dest='S')
        
#         # In newer fpdf versions, output might return bytes or string
#         if isinstance(pdf_str, str):
#             pdf_bytes = pdf_str.encode('latin-1')
#         else:
#             pdf_bytes = pdf_str

#         buffer = io.BytesIO(pdf_bytes)
#         buffer.seek(0)

#         return send_file(
#             buffer, 
#             mimetype='application/pdf', 
#             as_attachment=True, 
#             download_name="Truth_Report.pdf"
#         )

#     except Exception as e:
#         print(f"PDF GENERATION ERROR: {str(e)}")
#         return jsonify({"error": "Internal server error during PDF generation"}), 500

# @predict_bp.route("/scrape", methods=["POST"])
# def scrape_url():
#     data = request.get_json()
#     url = data.get("url")
    
#     headers = {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
#     }

#     try:
#         session = requests.Session()
#         response = session.get(url, headers=headers, timeout=20, verify=False, allow_redirects=True)
        
#         if response.status_code != 200:
#             return jsonify({"error": f"Site blocked us (Status: {response.status_code})"}), 400

#         article = Article(url)
#         article.set_html(response.text)
#         article.parse()
        
#         if not article.text or len(article.text) < 50:
#              if article.title:
#                  return jsonify({"text": "Content blocked, title found: " + article.title, "title": article.title})
#              return jsonify({"error": "No text found on page."}), 400
             
#         return jsonify({"text": article.text, "title": article.title})
        
#     except Exception as e:
#         print(f"SCRAPE ERROR: {str(e)}")
#         return jsonify({"error": "Connection failed. Try a different news source."}), 500

# @predict_bp.route("/predict", methods=["POST"])
# def predict():
#     data = request.get_json()
#     text = data.get("text", "")
    
#     if not text:
#         return jsonify({"error": "No text provided"}), 400

#     google_result = get_google_fact_check(text)
    
#     X = vectorizer.transform([text])
#     pred = int(model.predict(X)[0])
#     prob = model.predict_proba(X)[0]
    
#     feature_names = vectorizer.get_feature_names_out()
#     tfidf_scores = X.toarray()[0]
#     top_indices = tfidf_scores.argsort()[-5:][::-1]
#     top_keywords = [feature_names[i] for i in top_indices if tfidf_scores[i] > 0]

#     response = {
#         "prediction": "Real" if pred == 1 else "Fake",
#         "confidence": round(float(np.max(prob)), 2),
#         "top_keywords": top_keywords,
#         "external_verification": None
#     }

#     if google_result["found"]:
#         response["external_verification"] = {
#             "verdict": google_result["rating"],
#             "provider": google_result["source"],
#             "link": google_result["url"]
#         }
#         if any(word in google_result["rating"].lower() for word in ["false", "fake", "misleading", "incorrect"]):
#             response["prediction"] = "Fake"
#             response["confidence"] = 1.0

#     return jsonify(response)


from flask import Blueprint, request, jsonify, send_file
import pickle
import os
import numpy as np
import requests
import urllib3
from newspaper import Article
from fpdf import FPDF
import io
from deep_translator import GoogleTranslator
from langdetect import detect, DetectorFactory

# Ensures consistent language detection results
DetectorFactory.seed = 0

# Suppress insecure request warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

predict_bp = Blueprint("predict", __name__)

# --- Configuration ---
GOOGLE_API_KEY = "YAIzaSyCaFdRx30Pfow1od2o7lAcGsfgQjJ8NJ9E" 
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(BASE_DIR, "ml_dl", "fake_news_model.pkl")
VECTORIZER_PATH = os.path.join(BASE_DIR, "ml_dl", "tfidf_vectorizer.pkl")

# Load model & vectorizer
with open(MODEL_PATH, "rb") as f: model = pickle.load(f)
with open(VECTORIZER_PATH, "rb") as f: vectorizer = pickle.load(f)

def get_google_fact_check(query):
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search"
    params = {"query": query[:200], "key": GOOGLE_API_KEY}
    try:
        response = requests.get(url, params=params)
        data = response.json()
        if "claims" in data and len(data["claims"]) > 0:
            claim = data["claims"][0]
            review = claim["claimReview"][0]
            return {
                "found": True,
                "rating": review["textualRating"],
                "source": review["publisher"]["name"],
                "url": review["url"]
            }
    except: pass
    return {"found": False}

def clean_for_pdf(text):
    if not text: return ""
    return str(text).encode('latin-1', 'ignore').decode('latin-1')

@predict_bp.route("/generate_report", methods=["POST"])
def generate_report():
    try:
        data = request.get_json()
        text = data.get("text", "No text provided")
        prediction = data.get("prediction", "Unknown")
        confidence = data.get("confidence", 0)
        keywords = data.get("keywords", [])
        ext_ver = data.get("external_verification")
        verdict = ext_ver.get("verdict", "N/A") if ext_ver else "N/A"

        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", 'B', 24)
        pdf.set_text_color(37, 99, 235) 
        pdf.cell(190, 20, "TRUTH ENGINE ANALYSIS REPORT", ln=True, align='C')
        
        pdf.ln(10)
        pdf.set_fill_color(240, 240, 240)
        pdf.set_font("Arial", 'B', 16)
        pdf.set_text_color(0, 0, 0)
        pdf.cell(0, 15, f"FINAL VERDICT: {clean_for_pdf(prediction.upper())}", ln=True, align='C', fill=True)
        
        pdf.ln(10)
        pdf.set_font("Arial", 'B', 12)
        pdf.cell(0, 10, f"AI Confidence Score: {round(confidence * 100, 2)}%", ln=True)
        pdf.cell(0, 10, f"External Fact-Check: {clean_for_pdf(verdict)}", ln=True)
        
        kw_str = ", ".join(keywords) if keywords else "None detected"
        pdf.multi_cell(0, 10, f"Key Indicators: {clean_for_pdf(kw_str)}")
        
        pdf.ln(10)
        pdf.set_font("Arial", 'B', 14)
        pdf.set_text_color(37, 99, 235)
        pdf.cell(0, 10, "Analyzed Content Snippet:", ln=True)
        pdf.set_font("Arial", '', 10)
        pdf.set_text_color(80, 80, 80)
        content_snippet = clean_for_pdf(text[:1200]) + "..."
        pdf.multi_cell(0, 6, content_snippet)
        
        pdf_bytes = pdf.output(dest='S')
        if isinstance(pdf_bytes, str): pdf_bytes = pdf_bytes.encode('latin-1')

        buffer = io.BytesIO(pdf_bytes)
        buffer.seek(0)
        return send_file(buffer, mimetype='application/pdf', as_attachment=True, download_name="Truth_Report.pdf")
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@predict_bp.route("/scrape", methods=["POST"])
def scrape_url():
    data = request.get_json()
    url = data.get("url")
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        response = requests.get(url, headers=headers, timeout=20, verify=False)
        article = Article(url)
        article.set_html(response.text)
        article.parse()
        return jsonify({"text": article.text, "title": article.title})
    except Exception as e:
        return jsonify({"error": "Scrape failed"}), 500

# @predict_bp.route("/predict", methods=["POST"])
# def predict():
#     data = request.get_json()
#     text = data.get("text", "")
    
#     if not text:
#         return jsonify({"error": "No text provided"}), 400

#     try:
#         # 1. LANGUAGE DETECTION & TRANSLATION
#         source_lang = detect(text)
#         is_translated = False
#         analysis_text = text

#         if source_lang != 'en':
#             analysis_text = GoogleTranslator(source='auto', target='en').translate(text)
#             is_translated = True

#         # 2. GOOGLE FACT CHECK (Search using English version for better results)
#         google_result = get_google_fact_check(analysis_text)
        
#         # 3. ML MODEL PREDICTION
#         X = vectorizer.transform([analysis_text])
#         pred = int(model.predict(X)[0])
#         prob = model.predict_proba(X)[0]
        
#         # 4. KEYWORD EXTRACTION
#         feature_names = vectorizer.get_feature_names_out()
#         tfidf_scores = X.toarray()[0]
#         top_indices = tfidf_scores.argsort()[-5:][::-1]
#         top_keywords = [feature_names[i] for i in top_indices if tfidf_scores[i] > 0]

#         response = {
#             "prediction": "Real" if pred == 1 else "Fake",
#             "confidence": round(float(np.max(prob)), 2),
#             "top_keywords": top_keywords,
#             "language_meta": {
#                 "detected": source_lang,
#                 "translated": is_translated
#             },
#             "external_verification": None
#         }

#         # Override with Fact Check if available
#         if google_result["found"]:
#             response["external_verification"] = {
#                 "verdict": google_result["rating"],
#                 "provider": google_result["source"],
#                 "link": google_result["url"]
#             }
#             if any(word in google_result["rating"].lower() for word in ["false", "fake", "misleading", "incorrect"]):
#                 response["prediction"] = "Fake"
#                 response["confidence"] = 1.0

#         return jsonify(response)
#     except Exception as e:
#         print(f"PREDICT ERROR: {e}")
#         return jsonify({"error": "Neural engine error during prediction"}), 500

@predict_bp.route("/predict", methods=["POST"])
def predict():
    data = request.get_json()
    # Explicitly handle potential unicode issues
    text = data.get("text", "").strip()
    
    if not text:
        return jsonify({"error": "No text provided"}), 400

    try:
        # 1. LANGUAGE DETECTION
        # Hindi text will return 'hi'
        source_lang = detect(text)
        is_translated = False
        analysis_text = text

        # 2. THE NEURAL BRIDGE (Critical for Hindi)
        if source_lang != 'en':
            try:
                # Using 'auto' as source to ensure Hindi is caught
                analysis_text = GoogleTranslator(source='auto', target='en').translate(text)
                is_translated = True
                print(f"DEBUG: Translated {source_lang} to English: {analysis_text[:50]}...")
            except Exception as translation_err:
                print(f"TRANSLATION ERROR: {translation_err}")
                return jsonify({"error": "Translation service timed out. Please try again."}), 503

        # 3. ML MODEL PREDICTION (Always on English version)
        X = vectorizer.transform([analysis_text])
        pred = int(model.predict(X)[0])
        prob = model.predict_proba(X)[0]
        
        # 4. KEYWORD EXTRACTION
        feature_names = vectorizer.get_feature_names_out()
        tfidf_scores = X.toarray()[0]
        top_indices = tfidf_scores.argsort()[-5:][::-1]
        top_keywords = [feature_names[i] for i in top_indices if tfidf_scores[i] > 0]

        # 5. GOOGLE FACT CHECK
        google_result = get_google_fact_check(analysis_text)

        response = {
            "prediction": "Real" if pred == 1 else "Fake",
            "confidence": round(float(np.max(prob)), 2),
            "top_keywords": top_keywords,
            "language_meta": {
                "detected": source_lang,
                "translated": is_translated,
                "translated_text_preview": analysis_text[:100] # For debugging
            },
            "external_verification": None
        }

        # Override with Fact Check if available
        if google_result["found"]:
            response["external_verification"] = {
                "verdict": google_result["rating"],
                "provider": google_result["source"],
                "link": google_result["url"]
            }
            if any(word in google_result["rating"].lower() for word in ["false", "fake", "misleading", "incorrect"]):
                response["prediction"] = "Fake"
                response["confidence"] = 1.0

        return jsonify(response)

    except Exception as e:
        print(f"PREDICT ERROR: {str(e)}")
        return jsonify({"error": f"Internal processing error: {str(e)}"}), 500